{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 24.521344506470527,
  "eval_steps": 500,
  "global_step": 10650000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06,
      "grad_norm": 4.334728717803955,
      "learning_rate": 4.998561470361374e-05,
      "loss": 6.0284,
      "step": 25000
    },
    {
      "epoch": 0.12,
      "grad_norm": 4.8055243492126465,
      "learning_rate": 4.9971229407227474e-05,
      "loss": 4.8612,
      "step": 50000
    },
    {
      "epoch": 0.17,
      "grad_norm": 5.01570987701416,
      "learning_rate": 4.995684526207937e-05,
      "loss": 4.4625,
      "step": 75000
    },
    {
      "epoch": 0.23,
      "grad_norm": 5.6866254806518555,
      "learning_rate": 4.9942461116931264e-05,
      "loss": 4.23,
      "step": 100000
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.094673156738281,
      "learning_rate": 4.9928076971783155e-05,
      "loss": 4.076,
      "step": 125000
    },
    {
      "epoch": 0.35,
      "grad_norm": 5.467461585998535,
      "learning_rate": 4.991369225101597e-05,
      "loss": 3.9653,
      "step": 150000
    },
    {
      "epoch": 0.4,
      "grad_norm": 5.008171081542969,
      "learning_rate": 4.98993069546297e-05,
      "loss": 3.8802,
      "step": 175000
    },
    {
      "epoch": 0.46,
      "grad_norm": 4.978404998779297,
      "learning_rate": 4.98849228094816e-05,
      "loss": 3.8133,
      "step": 200000
    },
    {
      "epoch": 0.52,
      "grad_norm": 5.972187042236328,
      "learning_rate": 4.9870538088714416e-05,
      "loss": 3.7569,
      "step": 225000
    },
    {
      "epoch": 0.58,
      "grad_norm": 5.032392501831055,
      "learning_rate": 4.9856153367947226e-05,
      "loss": 3.7107,
      "step": 250000
    },
    {
      "epoch": 0.63,
      "grad_norm": 5.637373447418213,
      "learning_rate": 4.984176864718004e-05,
      "loss": 3.6731,
      "step": 275000
    },
    {
      "epoch": 0.69,
      "grad_norm": 5.798671245574951,
      "learning_rate": 4.9827383350793786e-05,
      "loss": 3.6355,
      "step": 300000
    },
    {
      "epoch": 0.75,
      "grad_norm": 5.73774528503418,
      "learning_rate": 4.9812998630026596e-05,
      "loss": 3.6029,
      "step": 325000
    },
    {
      "epoch": 0.81,
      "grad_norm": 5.669999122619629,
      "learning_rate": 4.979861390925941e-05,
      "loss": 3.5763,
      "step": 350000
    },
    {
      "epoch": 0.86,
      "grad_norm": 5.856439590454102,
      "learning_rate": 4.978422861287315e-05,
      "loss": 3.5489,
      "step": 375000
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.975999355316162,
      "learning_rate": 4.976984389210596e-05,
      "loss": 3.5286,
      "step": 400000
    },
    {
      "epoch": 0.98,
      "grad_norm": 5.727177143096924,
      "learning_rate": 4.9755459171338776e-05,
      "loss": 3.5079,
      "step": 425000
    },
    {
      "epoch": 1.04,
      "grad_norm": 5.939171314239502,
      "learning_rate": 4.974107329933344e-05,
      "loss": 3.4818,
      "step": 450000
    },
    {
      "epoch": 1.09,
      "grad_norm": 6.605532169342041,
      "learning_rate": 4.972668742732809e-05,
      "loss": 3.463,
      "step": 475000
    },
    {
      "epoch": 1.15,
      "grad_norm": 5.844452857971191,
      "learning_rate": 4.971230328217999e-05,
      "loss": 3.4463,
      "step": 500000
    },
    {
      "epoch": 1.21,
      "grad_norm": 5.826369285583496,
      "learning_rate": 4.9697917410174645e-05,
      "loss": 3.4344,
      "step": 525000
    },
    {
      "epoch": 1.27,
      "grad_norm": 6.090889930725098,
      "learning_rate": 4.968353211378838e-05,
      "loss": 3.4195,
      "step": 550000
    },
    {
      "epoch": 1.32,
      "grad_norm": 6.318431377410889,
      "learning_rate": 4.966914796864027e-05,
      "loss": 3.4073,
      "step": 575000
    },
    {
      "epoch": 1.38,
      "grad_norm": 5.603076934814453,
      "learning_rate": 4.965476209663493e-05,
      "loss": 3.3948,
      "step": 600000
    },
    {
      "epoch": 1.44,
      "grad_norm": 5.799454212188721,
      "learning_rate": 4.964037737586775e-05,
      "loss": 3.3834,
      "step": 625000
    },
    {
      "epoch": 1.5,
      "grad_norm": 5.940269947052002,
      "learning_rate": 4.9625992079481486e-05,
      "loss": 3.3717,
      "step": 650000
    },
    {
      "epoch": 1.55,
      "grad_norm": 5.531012535095215,
      "learning_rate": 4.961160620747614e-05,
      "loss": 3.3599,
      "step": 675000
    },
    {
      "epoch": 1.61,
      "grad_norm": 5.425093650817871,
      "learning_rate": 4.959722206232804e-05,
      "loss": 3.3494,
      "step": 700000
    },
    {
      "epoch": 1.67,
      "grad_norm": 6.111166000366211,
      "learning_rate": 4.958283676594177e-05,
      "loss": 3.3389,
      "step": 725000
    },
    {
      "epoch": 1.73,
      "grad_norm": 6.555903911590576,
      "learning_rate": 4.956845319641275e-05,
      "loss": 3.3312,
      "step": 750000
    },
    {
      "epoch": 1.78,
      "grad_norm": 5.388164520263672,
      "learning_rate": 4.955406674878833e-05,
      "loss": 3.3201,
      "step": 775000
    },
    {
      "epoch": 1.84,
      "grad_norm": 6.4102349281311035,
      "learning_rate": 4.9539681452402056e-05,
      "loss": 3.3113,
      "step": 800000
    },
    {
      "epoch": 1.9,
      "grad_norm": 6.185477256774902,
      "learning_rate": 4.9525297307253955e-05,
      "loss": 3.306,
      "step": 825000
    },
    {
      "epoch": 1.96,
      "grad_norm": 5.814029216766357,
      "learning_rate": 4.9510911435248616e-05,
      "loss": 3.2973,
      "step": 850000
    },
    {
      "epoch": 2.01,
      "grad_norm": 5.449276924133301,
      "learning_rate": 4.9496526714481426e-05,
      "loss": 3.2841,
      "step": 875000
    },
    {
      "epoch": 2.07,
      "grad_norm": 5.929284572601318,
      "learning_rate": 4.9482140266857006e-05,
      "loss": 3.2749,
      "step": 900000
    },
    {
      "epoch": 2.13,
      "grad_norm": 6.46092414855957,
      "learning_rate": 4.9467756121708904e-05,
      "loss": 3.2704,
      "step": 925000
    },
    {
      "epoch": 2.19,
      "grad_norm": 5.937037944793701,
      "learning_rate": 4.945337024970356e-05,
      "loss": 3.2697,
      "step": 950000
    },
    {
      "epoch": 2.24,
      "grad_norm": 5.515829563140869,
      "learning_rate": 4.9438984953317295e-05,
      "loss": 3.2623,
      "step": 975000
    },
    {
      "epoch": 2.3,
      "grad_norm": 5.34720516204834,
      "learning_rate": 4.9424597930073794e-05,
      "loss": 3.2535,
      "step": 1000000
    },
    {
      "epoch": 2.36,
      "grad_norm": 6.828314304351807,
      "learning_rate": 4.9410212058068455e-05,
      "loss": 3.2476,
      "step": 1025000
    },
    {
      "epoch": 2.42,
      "grad_norm": 6.392201900482178,
      "learning_rate": 4.939582733730127e-05,
      "loss": 3.2405,
      "step": 1050000
    },
    {
      "epoch": 2.48,
      "grad_norm": 6.083794116973877,
      "learning_rate": 4.9381443192153163e-05,
      "loss": 3.2424,
      "step": 1075000
    },
    {
      "epoch": 2.53,
      "grad_norm": 6.296161651611328,
      "learning_rate": 4.9367056744528744e-05,
      "loss": 3.237,
      "step": 1100000
    },
    {
      "epoch": 2.59,
      "grad_norm": 6.0752058029174805,
      "learning_rate": 4.935267144814248e-05,
      "loss": 3.2318,
      "step": 1125000
    },
    {
      "epoch": 2.65,
      "grad_norm": 5.919907093048096,
      "learning_rate": 4.933828500051806e-05,
      "loss": 3.2243,
      "step": 1150000
    },
    {
      "epoch": 2.71,
      "grad_norm": 6.626039981842041,
      "learning_rate": 4.932390085536995e-05,
      "loss": 3.2258,
      "step": 1175000
    },
    {
      "epoch": 2.76,
      "grad_norm": 5.86473274230957,
      "learning_rate": 4.930951498336461e-05,
      "loss": 3.2136,
      "step": 1200000
    },
    {
      "epoch": 2.82,
      "grad_norm": 6.178130626678467,
      "learning_rate": 4.929513026259743e-05,
      "loss": 3.2129,
      "step": 1225000
    },
    {
      "epoch": 2.88,
      "grad_norm": 5.908911228179932,
      "learning_rate": 4.928074554183024e-05,
      "loss": 3.2111,
      "step": 1250000
    },
    {
      "epoch": 2.94,
      "grad_norm": 6.420083045959473,
      "learning_rate": 4.9266360821063056e-05,
      "loss": 3.2066,
      "step": 1275000
    },
    {
      "epoch": 2.99,
      "grad_norm": 6.177522659301758,
      "learning_rate": 4.9251974373438637e-05,
      "loss": 3.2056,
      "step": 1300000
    },
    {
      "epoch": 3.05,
      "grad_norm": 5.825097560882568,
      "learning_rate": 4.9237589652671454e-05,
      "loss": 3.1984,
      "step": 1325000
    },
    {
      "epoch": 3.11,
      "grad_norm": 5.614321708679199,
      "learning_rate": 4.922320378066611e-05,
      "loss": 3.1967,
      "step": 1350000
    },
    {
      "epoch": 3.17,
      "grad_norm": 6.734071731567383,
      "learning_rate": 4.9208819059898925e-05,
      "loss": 3.1918,
      "step": 1375000
    },
    {
      "epoch": 3.22,
      "grad_norm": 6.157294750213623,
      "learning_rate": 4.919443318789358e-05,
      "loss": 3.186,
      "step": 1400000
    },
    {
      "epoch": 3.28,
      "grad_norm": 6.117190837860107,
      "learning_rate": 4.918004731588824e-05,
      "loss": 3.1843,
      "step": 1425000
    },
    {
      "epoch": 3.34,
      "grad_norm": 6.116796016693115,
      "learning_rate": 4.916566201950198e-05,
      "loss": 3.1788,
      "step": 1450000
    },
    {
      "epoch": 3.4,
      "grad_norm": 5.452373027801514,
      "learning_rate": 4.915127844997295e-05,
      "loss": 3.1761,
      "step": 1475000
    },
    {
      "epoch": 3.45,
      "grad_norm": 6.343470573425293,
      "learning_rate": 4.913689257796761e-05,
      "loss": 3.1765,
      "step": 1500000
    },
    {
      "epoch": 3.51,
      "grad_norm": 6.232460021972656,
      "learning_rate": 4.9122506705962265e-05,
      "loss": 3.1816,
      "step": 1525000
    },
    {
      "epoch": 3.57,
      "grad_norm": 6.195145130157471,
      "learning_rate": 4.9108121985195075e-05,
      "loss": 3.1773,
      "step": 1550000
    },
    {
      "epoch": 3.63,
      "grad_norm": 6.632346153259277,
      "learning_rate": 4.909373611318974e-05,
      "loss": 3.1777,
      "step": 1575000
    },
    {
      "epoch": 3.68,
      "grad_norm": 5.716767311096191,
      "learning_rate": 4.9079351392422554e-05,
      "loss": 3.1846,
      "step": 1600000
    },
    {
      "epoch": 3.74,
      "grad_norm": 5.688429355621338,
      "learning_rate": 4.9064964944798134e-05,
      "loss": 3.1684,
      "step": 1625000
    },
    {
      "epoch": 3.8,
      "grad_norm": 5.999467372894287,
      "learning_rate": 4.905057907279279e-05,
      "loss": 3.1524,
      "step": 1650000
    },
    {
      "epoch": 3.86,
      "grad_norm": 6.484237194061279,
      "learning_rate": 4.903619492764469e-05,
      "loss": 3.1514,
      "step": 1675000
    },
    {
      "epoch": 3.91,
      "grad_norm": 6.332479000091553,
      "learning_rate": 4.902181078249658e-05,
      "loss": 3.1482,
      "step": 1700000
    },
    {
      "epoch": 3.97,
      "grad_norm": 6.019323348999023,
      "learning_rate": 4.9007425486110314e-05,
      "loss": 3.1452,
      "step": 1725000
    },
    {
      "epoch": 4.03,
      "grad_norm": 6.695402145385742,
      "learning_rate": 4.8993039614104975e-05,
      "loss": 3.1467,
      "step": 1750000
    },
    {
      "epoch": 4.09,
      "grad_norm": 7.339609622955322,
      "learning_rate": 4.8978654893337785e-05,
      "loss": 3.1351,
      "step": 1775000
    },
    {
      "epoch": 4.14,
      "grad_norm": 5.679553985595703,
      "learning_rate": 4.8964268445713366e-05,
      "loss": 3.1325,
      "step": 1800000
    },
    {
      "epoch": 4.2,
      "grad_norm": 6.228635787963867,
      "learning_rate": 4.894988257370803e-05,
      "loss": 3.1289,
      "step": 1825000
    },
    {
      "epoch": 4.26,
      "grad_norm": 7.074507713317871,
      "learning_rate": 4.8935497852940844e-05,
      "loss": 3.1249,
      "step": 1850000
    },
    {
      "epoch": 4.32,
      "grad_norm": 6.7094902992248535,
      "learning_rate": 4.89211119809355e-05,
      "loss": 3.1246,
      "step": 1875000
    },
    {
      "epoch": 4.37,
      "grad_norm": 6.733193874359131,
      "learning_rate": 4.8906726684549234e-05,
      "loss": 3.1222,
      "step": 1900000
    },
    {
      "epoch": 4.43,
      "grad_norm": 5.6340651512146,
      "learning_rate": 4.889234196378205e-05,
      "loss": 3.1233,
      "step": 1925000
    },
    {
      "epoch": 4.49,
      "grad_norm": 6.907467365264893,
      "learning_rate": 4.887795666739579e-05,
      "loss": 3.1173,
      "step": 1950000
    },
    {
      "epoch": 4.55,
      "grad_norm": 5.801764488220215,
      "learning_rate": 4.886357252224768e-05,
      "loss": 3.1196,
      "step": 1975000
    },
    {
      "epoch": 4.6,
      "grad_norm": 7.062735557556152,
      "learning_rate": 4.8849187225861414e-05,
      "loss": 3.1203,
      "step": 2000000
    },
    {
      "epoch": 4.66,
      "grad_norm": 6.197744846343994,
      "learning_rate": 4.8834801353856076e-05,
      "loss": 3.1138,
      "step": 2025000
    },
    {
      "epoch": 4.72,
      "grad_norm": 6.136274337768555,
      "learning_rate": 4.882041605746981e-05,
      "loss": 3.112,
      "step": 2050000
    },
    {
      "epoch": 4.78,
      "grad_norm": 7.2564568519592285,
      "learning_rate": 4.880603133670262e-05,
      "loss": 3.1111,
      "step": 2075000
    },
    {
      "epoch": 4.84,
      "grad_norm": 7.278351306915283,
      "learning_rate": 4.8791646040316364e-05,
      "loss": 3.107,
      "step": 2100000
    },
    {
      "epoch": 4.89,
      "grad_norm": 6.500388145446777,
      "learning_rate": 4.8777261895168256e-05,
      "loss": 3.1135,
      "step": 2125000
    },
    {
      "epoch": 4.95,
      "grad_norm": 6.731493949890137,
      "learning_rate": 4.876287659878199e-05,
      "loss": 3.1146,
      "step": 2150000
    },
    {
      "epoch": 5.01,
      "grad_norm": 6.019585132598877,
      "learning_rate": 4.874849072677665e-05,
      "loss": 3.1106,
      "step": 2175000
    },
    {
      "epoch": 5.07,
      "grad_norm": 6.354411602020264,
      "learning_rate": 4.873410543039039e-05,
      "loss": 3.098,
      "step": 2200000
    },
    {
      "epoch": 5.12,
      "grad_norm": 6.833847999572754,
      "learning_rate": 4.8719720134004124e-05,
      "loss": 3.095,
      "step": 2225000
    },
    {
      "epoch": 5.18,
      "grad_norm": 7.758787631988525,
      "learning_rate": 4.870533426199878e-05,
      "loss": 3.093,
      "step": 2250000
    },
    {
      "epoch": 5.24,
      "grad_norm": 7.471386432647705,
      "learning_rate": 4.869094896561252e-05,
      "loss": 3.0937,
      "step": 2275000
    },
    {
      "epoch": 5.3,
      "grad_norm": 5.998228549957275,
      "learning_rate": 4.867656482046441e-05,
      "loss": 3.0878,
      "step": 2300000
    },
    {
      "epoch": 5.35,
      "grad_norm": 6.591770648956299,
      "learning_rate": 4.866218009969722e-05,
      "loss": 3.0885,
      "step": 2325000
    },
    {
      "epoch": 5.41,
      "grad_norm": 6.827066421508789,
      "learning_rate": 4.8647794227691884e-05,
      "loss": 3.0896,
      "step": 2350000
    },
    {
      "epoch": 5.47,
      "grad_norm": 6.760183334350586,
      "learning_rate": 4.8633407780067464e-05,
      "loss": 3.0868,
      "step": 2375000
    },
    {
      "epoch": 5.53,
      "grad_norm": 6.311361312866211,
      "learning_rate": 4.861902248368121e-05,
      "loss": 3.0815,
      "step": 2400000
    },
    {
      "epoch": 5.58,
      "grad_norm": 6.512691020965576,
      "learning_rate": 4.860463833853309e-05,
      "loss": 3.0843,
      "step": 2425000
    },
    {
      "epoch": 5.64,
      "grad_norm": 6.259239196777344,
      "learning_rate": 4.859025361776591e-05,
      "loss": 3.08,
      "step": 2450000
    },
    {
      "epoch": 5.7,
      "grad_norm": 6.204173564910889,
      "learning_rate": 4.857586717014149e-05,
      "loss": 3.0811,
      "step": 2475000
    },
    {
      "epoch": 5.76,
      "grad_norm": 6.9549407958984375,
      "learning_rate": 4.8561482449374306e-05,
      "loss": 3.0789,
      "step": 2500000
    },
    {
      "epoch": 5.81,
      "grad_norm": 6.7581939697265625,
      "learning_rate": 4.854709715298804e-05,
      "loss": 3.0751,
      "step": 2525000
    },
    {
      "epoch": 5.87,
      "grad_norm": 6.234816551208496,
      "learning_rate": 4.85327112809827e-05,
      "loss": 3.0733,
      "step": 2550000
    },
    {
      "epoch": 5.93,
      "grad_norm": 6.998606204986572,
      "learning_rate": 4.851832598459644e-05,
      "loss": 3.072,
      "step": 2575000
    },
    {
      "epoch": 5.99,
      "grad_norm": 6.995606899261475,
      "learning_rate": 4.850394126382925e-05,
      "loss": 3.0714,
      "step": 2600000
    },
    {
      "epoch": 6.04,
      "grad_norm": 8.034707069396973,
      "learning_rate": 4.848955711868115e-05,
      "loss": 3.0654,
      "step": 2625000
    },
    {
      "epoch": 6.1,
      "grad_norm": 6.459548473358154,
      "learning_rate": 4.847517239791396e-05,
      "loss": 3.0637,
      "step": 2650000
    },
    {
      "epoch": 6.16,
      "grad_norm": 8.421130180358887,
      "learning_rate": 4.846078710152769e-05,
      "loss": 3.0612,
      "step": 2675000
    },
    {
      "epoch": 6.22,
      "grad_norm": 7.008692264556885,
      "learning_rate": 4.8446401229522354e-05,
      "loss": 3.0627,
      "step": 2700000
    },
    {
      "epoch": 6.27,
      "grad_norm": 7.103756427764893,
      "learning_rate": 4.843201593313609e-05,
      "loss": 3.0595,
      "step": 2725000
    },
    {
      "epoch": 6.33,
      "grad_norm": 6.649708271026611,
      "learning_rate": 4.8417630636749826e-05,
      "loss": 3.0606,
      "step": 2750000
    },
    {
      "epoch": 6.39,
      "grad_norm": 7.381323337554932,
      "learning_rate": 4.840324591598264e-05,
      "loss": 3.0593,
      "step": 2775000
    },
    {
      "epoch": 6.45,
      "grad_norm": 7.117290496826172,
      "learning_rate": 4.8388860043977304e-05,
      "loss": 3.0583,
      "step": 2800000
    },
    {
      "epoch": 6.5,
      "grad_norm": 6.329601287841797,
      "learning_rate": 4.837447474759103e-05,
      "loss": 3.056,
      "step": 2825000
    },
    {
      "epoch": 6.56,
      "grad_norm": 6.4717183113098145,
      "learning_rate": 4.836008829996661e-05,
      "loss": 3.0569,
      "step": 2850000
    },
    {
      "epoch": 6.62,
      "grad_norm": 6.417482376098633,
      "learning_rate": 4.8345702427961275e-05,
      "loss": 3.0522,
      "step": 2875000
    },
    {
      "epoch": 6.68,
      "grad_norm": 7.035141944885254,
      "learning_rate": 4.833131770719409e-05,
      "loss": 3.0538,
      "step": 2900000
    },
    {
      "epoch": 6.73,
      "grad_norm": 7.192385196685791,
      "learning_rate": 4.831693241080783e-05,
      "loss": 3.0556,
      "step": 2925000
    },
    {
      "epoch": 6.79,
      "grad_norm": 7.179018497467041,
      "learning_rate": 4.830254769004064e-05,
      "loss": 3.049,
      "step": 2950000
    },
    {
      "epoch": 6.85,
      "grad_norm": 6.76092004776001,
      "learning_rate": 4.82881618180353e-05,
      "loss": 3.0492,
      "step": 2975000
    },
    {
      "epoch": 6.91,
      "grad_norm": 7.415715217590332,
      "learning_rate": 4.8273777097268116e-05,
      "loss": 3.0527,
      "step": 3000000
    },
    {
      "epoch": 6.96,
      "grad_norm": 6.476744651794434,
      "learning_rate": 4.825939122526277e-05,
      "loss": 3.0477,
      "step": 3025000
    },
    {
      "epoch": 7.02,
      "grad_norm": 6.547642707824707,
      "learning_rate": 4.824500535325743e-05,
      "loss": 3.043,
      "step": 3050000
    },
    {
      "epoch": 7.08,
      "grad_norm": 8.653116226196289,
      "learning_rate": 4.823062005687117e-05,
      "loss": 3.0429,
      "step": 3075000
    },
    {
      "epoch": 7.14,
      "grad_norm": 8.063749313354492,
      "learning_rate": 4.8216234760484903e-05,
      "loss": 3.0427,
      "step": 3100000
    },
    {
      "epoch": 7.2,
      "grad_norm": 8.196056365966797,
      "learning_rate": 4.8201848888479565e-05,
      "loss": 3.0415,
      "step": 3125000
    },
    {
      "epoch": 7.25,
      "grad_norm": 9.881194114685059,
      "learning_rate": 4.8187464167712375e-05,
      "loss": 3.043,
      "step": 3150000
    },
    {
      "epoch": 7.31,
      "grad_norm": 8.70830249786377,
      "learning_rate": 4.817307887132612e-05,
      "loss": 3.0437,
      "step": 3175000
    },
    {
      "epoch": 7.37,
      "grad_norm": 9.431268692016602,
      "learning_rate": 4.815869357493985e-05,
      "loss": 3.045,
      "step": 3200000
    },
    {
      "epoch": 7.43,
      "grad_norm": 8.097981452941895,
      "learning_rate": 4.814430827855358e-05,
      "loss": 3.0483,
      "step": 3225000
    },
    {
      "epoch": 7.48,
      "grad_norm": 9.013763427734375,
      "learning_rate": 4.812992183092916e-05,
      "loss": 3.0446,
      "step": 3250000
    },
    {
      "epoch": 7.54,
      "grad_norm": 10.021117210388184,
      "learning_rate": 4.811553711016198e-05,
      "loss": 3.0406,
      "step": 3275000
    },
    {
      "epoch": 7.6,
      "grad_norm": 9.313142776489258,
      "learning_rate": 4.8101152389394796e-05,
      "loss": 3.0432,
      "step": 3300000
    },
    {
      "epoch": 7.66,
      "grad_norm": 8.998252868652344,
      "learning_rate": 4.808676651738945e-05,
      "loss": 3.0416,
      "step": 3325000
    },
    {
      "epoch": 7.71,
      "grad_norm": 10.704177856445312,
      "learning_rate": 4.807238237224135e-05,
      "loss": 3.0377,
      "step": 3350000
    },
    {
      "epoch": 7.77,
      "grad_norm": 9.913203239440918,
      "learning_rate": 4.805799592461693e-05,
      "loss": 3.0352,
      "step": 3375000
    },
    {
      "epoch": 7.83,
      "grad_norm": 12.091437339782715,
      "learning_rate": 4.804361177946882e-05,
      "loss": 3.0362,
      "step": 3400000
    },
    {
      "epoch": 7.89,
      "grad_norm": 9.906620979309082,
      "learning_rate": 4.8029226483082557e-05,
      "loss": 3.0351,
      "step": 3425000
    },
    {
      "epoch": 7.94,
      "grad_norm": 11.19762134552002,
      "learning_rate": 4.801483945983906e-05,
      "loss": 3.0359,
      "step": 3450000
    },
    {
      "epoch": 8.0,
      "grad_norm": 13.713910102844238,
      "learning_rate": 4.800045301221464e-05,
      "loss": 3.0306,
      "step": 3475000
    },
    {
      "epoch": 8.06,
      "grad_norm": 13.004206657409668,
      "learning_rate": 4.7986070593923764e-05,
      "loss": 3.0195,
      "step": 3500000
    },
    {
      "epoch": 8.12,
      "grad_norm": 12.181453704833984,
      "learning_rate": 4.7971684721918425e-05,
      "loss": 3.02,
      "step": 3525000
    },
    {
      "epoch": 8.17,
      "grad_norm": 13.239008903503418,
      "learning_rate": 4.795729942553216e-05,
      "loss": 3.0201,
      "step": 3550000
    },
    {
      "epoch": 8.23,
      "grad_norm": 12.541651725769043,
      "learning_rate": 4.794291470476498e-05,
      "loss": 3.0222,
      "step": 3575000
    },
    {
      "epoch": 8.29,
      "grad_norm": 16.861164093017578,
      "learning_rate": 4.792852883275963e-05,
      "loss": 3.0213,
      "step": 3600000
    },
    {
      "epoch": 8.35,
      "grad_norm": 14.561646461486816,
      "learning_rate": 4.791414411199245e-05,
      "loss": 3.0253,
      "step": 3625000
    },
    {
      "epoch": 8.4,
      "grad_norm": 16.53850555419922,
      "learning_rate": 4.7899758815606185e-05,
      "loss": 3.0229,
      "step": 3650000
    },
    {
      "epoch": 8.46,
      "grad_norm": 12.984368324279785,
      "learning_rate": 4.788537294360084e-05,
      "loss": 3.0191,
      "step": 3675000
    },
    {
      "epoch": 8.52,
      "grad_norm": 17.25298500061035,
      "learning_rate": 4.7870988222833664e-05,
      "loss": 3.0194,
      "step": 3700000
    },
    {
      "epoch": 8.58,
      "grad_norm": 15.828742980957031,
      "learning_rate": 4.785660235082832e-05,
      "loss": 3.0298,
      "step": 3725000
    },
    {
      "epoch": 8.63,
      "grad_norm": 15.047117233276367,
      "learning_rate": 4.7842217054442054e-05,
      "loss": 3.0216,
      "step": 3750000
    },
    {
      "epoch": 8.69,
      "grad_norm": 21.215007781982422,
      "learning_rate": 4.782783118243671e-05,
      "loss": 3.018,
      "step": 3775000
    },
    {
      "epoch": 8.75,
      "grad_norm": 17.361360549926758,
      "learning_rate": 4.781344531043137e-05,
      "loss": 3.0168,
      "step": 3800000
    },
    {
      "epoch": 8.81,
      "grad_norm": 14.142017364501953,
      "learning_rate": 4.779906058966419e-05,
      "loss": 3.0146,
      "step": 3825000
    },
    {
      "epoch": 8.86,
      "grad_norm": 14.431478500366211,
      "learning_rate": 4.7784675868897e-05,
      "loss": 3.016,
      "step": 3850000
    },
    {
      "epoch": 8.92,
      "grad_norm": 14.918367385864258,
      "learning_rate": 4.777029057251074e-05,
      "loss": 3.0166,
      "step": 3875000
    },
    {
      "epoch": 8.98,
      "grad_norm": 16.975418090820312,
      "learning_rate": 4.7755905276124475e-05,
      "loss": 3.0207,
      "step": 3900000
    },
    {
      "epoch": 9.04,
      "grad_norm": 11.753580093383789,
      "learning_rate": 4.774151940411914e-05,
      "loss": 3.0097,
      "step": 3925000
    },
    {
      "epoch": 9.09,
      "grad_norm": 13.436349868774414,
      "learning_rate": 4.772713353211379e-05,
      "loss": 3.0059,
      "step": 3950000
    },
    {
      "epoch": 9.15,
      "grad_norm": 13.579776763916016,
      "learning_rate": 4.771274938696568e-05,
      "loss": 3.0101,
      "step": 3975000
    },
    {
      "epoch": 9.21,
      "grad_norm": 14.077165603637695,
      "learning_rate": 4.769836293934126e-05,
      "loss": 3.0094,
      "step": 4000000
    },
    {
      "epoch": 9.27,
      "grad_norm": 18.01968002319336,
      "learning_rate": 4.7683977642955005e-05,
      "loss": 3.0135,
      "step": 4025000
    },
    {
      "epoch": 9.33,
      "grad_norm": 14.546148300170898,
      "learning_rate": 4.766959119533058e-05,
      "loss": 3.009,
      "step": 4050000
    },
    {
      "epoch": 9.38,
      "grad_norm": 13.997180938720703,
      "learning_rate": 4.7655206474563396e-05,
      "loss": 3.008,
      "step": 4075000
    },
    {
      "epoch": 9.44,
      "grad_norm": 15.327549934387207,
      "learning_rate": 4.764082117817713e-05,
      "loss": 3.0123,
      "step": 4100000
    },
    {
      "epoch": 9.5,
      "grad_norm": 11.835865020751953,
      "learning_rate": 4.762643473055271e-05,
      "loss": 3.0085,
      "step": 4125000
    },
    {
      "epoch": 9.56,
      "grad_norm": 15.84119701385498,
      "learning_rate": 4.761204943416645e-05,
      "loss": 3.0059,
      "step": 4150000
    },
    {
      "epoch": 9.61,
      "grad_norm": 13.669358253479004,
      "learning_rate": 4.759766586463742e-05,
      "loss": 3.0069,
      "step": 4175000
    },
    {
      "epoch": 9.67,
      "grad_norm": 13.672082901000977,
      "learning_rate": 4.7583279417013e-05,
      "loss": 3.0037,
      "step": 4200000
    },
    {
      "epoch": 9.73,
      "grad_norm": 18.94371223449707,
      "learning_rate": 4.7568895847483966e-05,
      "loss": 3.0094,
      "step": 4225000
    },
    {
      "epoch": 9.79,
      "grad_norm": 11.650430679321289,
      "learning_rate": 4.755450939985955e-05,
      "loss": 3.0244,
      "step": 4250000
    },
    {
      "epoch": 9.84,
      "grad_norm": 17.855379104614258,
      "learning_rate": 4.754012410347329e-05,
      "loss": 3.0108,
      "step": 4275000
    },
    {
      "epoch": 9.9,
      "grad_norm": 14.239798545837402,
      "learning_rate": 4.7525740533944255e-05,
      "loss": 3.0287,
      "step": 4300000
    },
    {
      "epoch": 9.96,
      "grad_norm": 12.11362361907959,
      "learning_rate": 4.7511354086319835e-05,
      "loss": 3.0223,
      "step": 4325000
    },
    {
      "epoch": 10.02,
      "grad_norm": 16.065549850463867,
      "learning_rate": 4.749696936555265e-05,
      "loss": 3.0158,
      "step": 4350000
    },
    {
      "epoch": 10.07,
      "grad_norm": 14.900140762329102,
      "learning_rate": 4.748258291792824e-05,
      "loss": 3.0106,
      "step": 4375000
    },
    {
      "epoch": 10.13,
      "grad_norm": 20.37098503112793,
      "learning_rate": 4.7468197621541974e-05,
      "loss": 3.0099,
      "step": 4400000
    },
    {
      "epoch": 10.19,
      "grad_norm": 25.554309844970703,
      "learning_rate": 4.7453812325155703e-05,
      "loss": 3.0087,
      "step": 4425000
    },
    {
      "epoch": 10.25,
      "grad_norm": 13.34979248046875,
      "learning_rate": 4.743942760438852e-05,
      "loss": 3.0042,
      "step": 4450000
    },
    {
      "epoch": 10.3,
      "grad_norm": 18.182514190673828,
      "learning_rate": 4.742504173238318e-05,
      "loss": 3.0159,
      "step": 4475000
    },
    {
      "epoch": 10.36,
      "grad_norm": 13.981024742126465,
      "learning_rate": 4.741065758723507e-05,
      "loss": 3.0212,
      "step": 4500000
    },
    {
      "epoch": 10.42,
      "grad_norm": 12.930743217468262,
      "learning_rate": 4.739627229084881e-05,
      "loss": 3.0114,
      "step": 4525000
    },
    {
      "epoch": 10.48,
      "grad_norm": 14.377760887145996,
      "learning_rate": 4.7381887570081626e-05,
      "loss": 3.0045,
      "step": 4550000
    },
    {
      "epoch": 10.53,
      "grad_norm": 25.194839477539062,
      "learning_rate": 4.7367502849314436e-05,
      "loss": 3.0121,
      "step": 4575000
    },
    {
      "epoch": 10.59,
      "grad_norm": 14.739550590515137,
      "learning_rate": 4.7353116401690016e-05,
      "loss": 3.0069,
      "step": 4600000
    },
    {
      "epoch": 10.65,
      "grad_norm": 10.793962478637695,
      "learning_rate": 4.7338729954065596e-05,
      "loss": 2.9992,
      "step": 4625000
    },
    {
      "epoch": 10.71,
      "grad_norm": 21.431787490844727,
      "learning_rate": 4.7324347535774725e-05,
      "loss": 3.0062,
      "step": 4650000
    },
    {
      "epoch": 10.76,
      "grad_norm": 12.068631172180176,
      "learning_rate": 4.7309961088150305e-05,
      "loss": 3.003,
      "step": 4675000
    },
    {
      "epoch": 10.82,
      "grad_norm": 13.547396659851074,
      "learning_rate": 4.729557579176405e-05,
      "loss": 2.9987,
      "step": 4700000
    },
    {
      "epoch": 10.88,
      "grad_norm": 13.062262535095215,
      "learning_rate": 4.728119049537778e-05,
      "loss": 3.0005,
      "step": 4725000
    },
    {
      "epoch": 10.94,
      "grad_norm": 16.571243286132812,
      "learning_rate": 4.726680519899151e-05,
      "loss": 3.0008,
      "step": 4750000
    },
    {
      "epoch": 10.99,
      "grad_norm": 13.285433769226074,
      "learning_rate": 4.72524187513671e-05,
      "loss": 3.0004,
      "step": 4775000
    },
    {
      "epoch": 11.05,
      "grad_norm": 16.58223533630371,
      "learning_rate": 4.7238034030599916e-05,
      "loss": 2.9935,
      "step": 4800000
    },
    {
      "epoch": 11.11,
      "grad_norm": 11.095519065856934,
      "learning_rate": 4.722364815859457e-05,
      "loss": 2.9911,
      "step": 4825000
    },
    {
      "epoch": 11.17,
      "grad_norm": 11.09562873840332,
      "learning_rate": 4.720926228658923e-05,
      "loss": 2.9848,
      "step": 4850000
    },
    {
      "epoch": 11.22,
      "grad_norm": 11.625791549682617,
      "learning_rate": 4.719487814144112e-05,
      "loss": 2.9892,
      "step": 4875000
    },
    {
      "epoch": 11.28,
      "grad_norm": 11.908024787902832,
      "learning_rate": 4.718049342067394e-05,
      "loss": 2.9902,
      "step": 4900000
    },
    {
      "epoch": 11.34,
      "grad_norm": 12.450258255004883,
      "learning_rate": 4.7166109851144906e-05,
      "loss": 2.9919,
      "step": 4925000
    },
    {
      "epoch": 11.4,
      "grad_norm": 15.190801620483398,
      "learning_rate": 4.715172397913957e-05,
      "loss": 2.9902,
      "step": 4950000
    },
    {
      "epoch": 11.45,
      "grad_norm": 10.877873420715332,
      "learning_rate": 4.7137339258372385e-05,
      "loss": 2.9917,
      "step": 4975000
    },
    {
      "epoch": 11.51,
      "grad_norm": 15.849689483642578,
      "learning_rate": 4.712295281074796e-05,
      "loss": 2.9834,
      "step": 5000000
    },
    {
      "epoch": 11.57,
      "grad_norm": 15.883771896362305,
      "learning_rate": 4.7108567514361694e-05,
      "loss": 2.9872,
      "step": 5025000
    },
    {
      "epoch": 11.63,
      "grad_norm": 12.648734092712402,
      "learning_rate": 4.709418279359452e-05,
      "loss": 2.9863,
      "step": 5050000
    },
    {
      "epoch": 11.69,
      "grad_norm": 27.599445343017578,
      "learning_rate": 4.707979749720825e-05,
      "loss": 2.9857,
      "step": 5075000
    },
    {
      "epoch": 11.74,
      "grad_norm": 14.119174003601074,
      "learning_rate": 4.7065412776441063e-05,
      "loss": 2.9852,
      "step": 5100000
    },
    {
      "epoch": 11.8,
      "grad_norm": 12.140446662902832,
      "learning_rate": 4.7051026904435725e-05,
      "loss": 2.983,
      "step": 5125000
    },
    {
      "epoch": 11.86,
      "grad_norm": 20.88338279724121,
      "learning_rate": 4.7036642759287616e-05,
      "loss": 2.9847,
      "step": 5150000
    },
    {
      "epoch": 11.92,
      "grad_norm": 10.629908561706543,
      "learning_rate": 4.7022256311663196e-05,
      "loss": 2.983,
      "step": 5175000
    },
    {
      "epoch": 11.97,
      "grad_norm": 15.419696807861328,
      "learning_rate": 4.700787159089601e-05,
      "loss": 2.9826,
      "step": 5200000
    },
    {
      "epoch": 12.03,
      "grad_norm": 13.690133094787598,
      "learning_rate": 4.6993488021366986e-05,
      "loss": 2.9797,
      "step": 5225000
    },
    {
      "epoch": 12.09,
      "grad_norm": 9.87920093536377,
      "learning_rate": 4.6979102724980715e-05,
      "loss": 2.981,
      "step": 5250000
    },
    {
      "epoch": 12.15,
      "grad_norm": 12.839898109436035,
      "learning_rate": 4.6964716852975376e-05,
      "loss": 2.9756,
      "step": 5275000
    },
    {
      "epoch": 12.2,
      "grad_norm": 14.259954452514648,
      "learning_rate": 4.695033155658911e-05,
      "loss": 2.9758,
      "step": 5300000
    },
    {
      "epoch": 12.26,
      "grad_norm": 12.946834564208984,
      "learning_rate": 4.693594683582193e-05,
      "loss": 2.9777,
      "step": 5325000
    },
    {
      "epoch": 12.32,
      "grad_norm": 10.314701080322266,
      "learning_rate": 4.69215632662929e-05,
      "loss": 2.9768,
      "step": 5350000
    },
    {
      "epoch": 12.38,
      "grad_norm": 12.601819038391113,
      "learning_rate": 4.690717739428756e-05,
      "loss": 2.9717,
      "step": 5375000
    },
    {
      "epoch": 12.43,
      "grad_norm": 10.363166809082031,
      "learning_rate": 4.689279324913945e-05,
      "loss": 2.9745,
      "step": 5400000
    },
    {
      "epoch": 12.49,
      "grad_norm": 14.166749954223633,
      "learning_rate": 4.687840795275319e-05,
      "loss": 2.9729,
      "step": 5425000
    },
    {
      "epoch": 12.55,
      "grad_norm": 13.551831245422363,
      "learning_rate": 4.686402150512877e-05,
      "loss": 2.9675,
      "step": 5450000
    },
    {
      "epoch": 12.61,
      "grad_norm": 10.230145454406738,
      "learning_rate": 4.6849636208742506e-05,
      "loss": 2.9705,
      "step": 5475000
    },
    {
      "epoch": 12.66,
      "grad_norm": 10.539916038513184,
      "learning_rate": 4.683525033673716e-05,
      "loss": 2.9699,
      "step": 5500000
    },
    {
      "epoch": 12.72,
      "grad_norm": 12.438721656799316,
      "learning_rate": 4.682086619158906e-05,
      "loss": 2.9714,
      "step": 5525000
    },
    {
      "epoch": 12.78,
      "grad_norm": 10.620551109313965,
      "learning_rate": 4.6806480895202795e-05,
      "loss": 2.9734,
      "step": 5550000
    },
    {
      "epoch": 12.84,
      "grad_norm": 11.779677391052246,
      "learning_rate": 4.6792096174435605e-05,
      "loss": 2.9683,
      "step": 5575000
    },
    {
      "epoch": 12.89,
      "grad_norm": 8.83978271484375,
      "learning_rate": 4.6777709726811185e-05,
      "loss": 2.9676,
      "step": 5600000
    },
    {
      "epoch": 12.95,
      "grad_norm": 9.52343463897705,
      "learning_rate": 4.676332558166308e-05,
      "loss": 2.9692,
      "step": 5625000
    },
    {
      "epoch": 13.01,
      "grad_norm": 9.748608589172363,
      "learning_rate": 4.674893913403866e-05,
      "loss": 2.9651,
      "step": 5650000
    },
    {
      "epoch": 13.07,
      "grad_norm": 10.16627025604248,
      "learning_rate": 4.673455326203332e-05,
      "loss": 2.959,
      "step": 5675000
    },
    {
      "epoch": 13.12,
      "grad_norm": 9.764198303222656,
      "learning_rate": 4.6720168541266135e-05,
      "loss": 2.9576,
      "step": 5700000
    },
    {
      "epoch": 13.18,
      "grad_norm": 8.605834007263184,
      "learning_rate": 4.6705782669260796e-05,
      "loss": 2.9575,
      "step": 5725000
    },
    {
      "epoch": 13.24,
      "grad_norm": 11.118219375610352,
      "learning_rate": 4.669139737287453e-05,
      "loss": 2.956,
      "step": 5750000
    },
    {
      "epoch": 13.3,
      "grad_norm": 10.089524269104004,
      "learning_rate": 4.6677011500869187e-05,
      "loss": 2.9601,
      "step": 5775000
    },
    {
      "epoch": 13.35,
      "grad_norm": 10.56136703491211,
      "learning_rate": 4.666262620448292e-05,
      "loss": 2.9632,
      "step": 5800000
    },
    {
      "epoch": 13.41,
      "grad_norm": 13.33781623840332,
      "learning_rate": 4.66482397568585e-05,
      "loss": 2.9581,
      "step": 5825000
    },
    {
      "epoch": 13.47,
      "grad_norm": 9.075553894042969,
      "learning_rate": 4.663385503609132e-05,
      "loss": 2.9574,
      "step": 5850000
    },
    {
      "epoch": 13.53,
      "grad_norm": 15.362508773803711,
      "learning_rate": 4.6619469739705055e-05,
      "loss": 2.9602,
      "step": 5875000
    },
    {
      "epoch": 13.58,
      "grad_norm": 9.832100868225098,
      "learning_rate": 4.660508559455695e-05,
      "loss": 2.9559,
      "step": 5900000
    },
    {
      "epoch": 13.64,
      "grad_norm": 11.752697944641113,
      "learning_rate": 4.659070029817069e-05,
      "loss": 2.9601,
      "step": 5925000
    },
    {
      "epoch": 13.7,
      "grad_norm": 11.205643653869629,
      "learning_rate": 4.6576316728641655e-05,
      "loss": 2.9564,
      "step": 5950000
    },
    {
      "epoch": 13.76,
      "grad_norm": 8.77880859375,
      "learning_rate": 4.656192970539816e-05,
      "loss": 2.9562,
      "step": 5975000
    },
    {
      "epoch": 13.81,
      "grad_norm": 9.690605163574219,
      "learning_rate": 4.6547544409011897e-05,
      "loss": 2.9585,
      "step": 6000000
    },
    {
      "epoch": 13.87,
      "grad_norm": 10.693437576293945,
      "learning_rate": 4.653316026386379e-05,
      "loss": 2.9555,
      "step": 6025000
    },
    {
      "epoch": 13.93,
      "grad_norm": 8.330328941345215,
      "learning_rate": 4.6518774967477524e-05,
      "loss": 2.9587,
      "step": 6050000
    },
    {
      "epoch": 13.99,
      "grad_norm": 8.924124717712402,
      "learning_rate": 4.650438967109126e-05,
      "loss": 2.9533,
      "step": 6075000
    },
    {
      "epoch": 14.05,
      "grad_norm": 8.638691902160645,
      "learning_rate": 4.649000379908592e-05,
      "loss": 2.9491,
      "step": 6100000
    },
    {
      "epoch": 14.1,
      "grad_norm": 10.882630348205566,
      "learning_rate": 4.6475617927080575e-05,
      "loss": 2.947,
      "step": 6125000
    },
    {
      "epoch": 14.16,
      "grad_norm": 8.676663398742676,
      "learning_rate": 4.6461233781932474e-05,
      "loss": 2.9491,
      "step": 6150000
    },
    {
      "epoch": 14.22,
      "grad_norm": 12.855329513549805,
      "learning_rate": 4.644684906116529e-05,
      "loss": 2.9502,
      "step": 6175000
    },
    {
      "epoch": 14.28,
      "grad_norm": 8.740622520446777,
      "learning_rate": 4.64324643403981e-05,
      "loss": 2.9464,
      "step": 6200000
    },
    {
      "epoch": 14.33,
      "grad_norm": 10.171278953552246,
      "learning_rate": 4.64180773171546e-05,
      "loss": 2.9534,
      "step": 6225000
    },
    {
      "epoch": 14.39,
      "grad_norm": 9.636786460876465,
      "learning_rate": 4.640369259638742e-05,
      "loss": 2.9492,
      "step": 6250000
    },
    {
      "epoch": 14.45,
      "grad_norm": 9.375378608703613,
      "learning_rate": 4.638930845123931e-05,
      "loss": 2.9481,
      "step": 6275000
    },
    {
      "epoch": 14.51,
      "grad_norm": 12.735394477844238,
      "learning_rate": 4.637492257923397e-05,
      "loss": 2.954,
      "step": 6300000
    },
    {
      "epoch": 14.56,
      "grad_norm": 9.296468734741211,
      "learning_rate": 4.6360537858466786e-05,
      "loss": 2.9586,
      "step": 6325000
    },
    {
      "epoch": 14.62,
      "grad_norm": 9.947601318359375,
      "learning_rate": 4.634615198646144e-05,
      "loss": 2.9604,
      "step": 6350000
    },
    {
      "epoch": 14.68,
      "grad_norm": 9.49439525604248,
      "learning_rate": 4.6331766114456096e-05,
      "loss": 2.9556,
      "step": 6375000
    },
    {
      "epoch": 14.74,
      "grad_norm": 10.708386421203613,
      "learning_rate": 4.631738081806984e-05,
      "loss": 2.9536,
      "step": 6400000
    },
    {
      "epoch": 14.79,
      "grad_norm": 9.277993202209473,
      "learning_rate": 4.6302995521683574e-05,
      "loss": 2.9523,
      "step": 6425000
    },
    {
      "epoch": 14.85,
      "grad_norm": 12.52829647064209,
      "learning_rate": 4.628861080091639e-05,
      "loss": 2.9527,
      "step": 6450000
    },
    {
      "epoch": 14.91,
      "grad_norm": 9.886188507080078,
      "learning_rate": 4.627422550453012e-05,
      "loss": 2.9517,
      "step": 6475000
    },
    {
      "epoch": 14.97,
      "grad_norm": 13.48583698272705,
      "learning_rate": 4.62598390569057e-05,
      "loss": 2.9468,
      "step": 6500000
    },
    {
      "epoch": 15.02,
      "grad_norm": 8.65366268157959,
      "learning_rate": 4.6245454336138524e-05,
      "loss": 2.9604,
      "step": 6525000
    },
    {
      "epoch": 15.08,
      "grad_norm": 13.47224235534668,
      "learning_rate": 4.623106846413318e-05,
      "loss": 2.944,
      "step": 6550000
    },
    {
      "epoch": 15.14,
      "grad_norm": 9.835408210754395,
      "learning_rate": 4.621668259212783e-05,
      "loss": 2.9554,
      "step": 6575000
    },
    {
      "epoch": 15.2,
      "grad_norm": 9.897438049316406,
      "learning_rate": 4.620229614450341e-05,
      "loss": 2.9401,
      "step": 6600000
    },
    {
      "epoch": 15.25,
      "grad_norm": 11.2277250289917,
      "learning_rate": 4.618791199935531e-05,
      "loss": 2.9449,
      "step": 6625000
    },
    {
      "epoch": 15.31,
      "grad_norm": 8.191061019897461,
      "learning_rate": 4.617352612734997e-05,
      "loss": 2.9376,
      "step": 6650000
    },
    {
      "epoch": 15.37,
      "grad_norm": 9.151451110839844,
      "learning_rate": 4.615914140658278e-05,
      "loss": 2.9424,
      "step": 6675000
    },
    {
      "epoch": 15.43,
      "grad_norm": 11.469039916992188,
      "learning_rate": 4.614475495895836e-05,
      "loss": 2.9372,
      "step": 6700000
    },
    {
      "epoch": 15.48,
      "grad_norm": 10.265222549438477,
      "learning_rate": 4.61303696625721e-05,
      "loss": 2.9392,
      "step": 6725000
    },
    {
      "epoch": 15.54,
      "grad_norm": 10.58554458618164,
      "learning_rate": 4.6115984941804916e-05,
      "loss": 2.9421,
      "step": 6750000
    },
    {
      "epoch": 15.6,
      "grad_norm": 8.637203216552734,
      "learning_rate": 4.610159964541865e-05,
      "loss": 2.9421,
      "step": 6775000
    },
    {
      "epoch": 15.66,
      "grad_norm": 35.39568328857422,
      "learning_rate": 4.608721550027054e-05,
      "loss": 2.9418,
      "step": 6800000
    },
    {
      "epoch": 15.71,
      "grad_norm": 10.028321266174316,
      "learning_rate": 4.607282905264612e-05,
      "loss": 2.9387,
      "step": 6825000
    },
    {
      "epoch": 15.77,
      "grad_norm": 9.435445785522461,
      "learning_rate": 4.605844433187893e-05,
      "loss": 2.9322,
      "step": 6850000
    },
    {
      "epoch": 15.83,
      "grad_norm": 9.596477508544922,
      "learning_rate": 4.6044058459873595e-05,
      "loss": 2.9322,
      "step": 6875000
    },
    {
      "epoch": 15.89,
      "grad_norm": 9.131564140319824,
      "learning_rate": 4.602967316348733e-05,
      "loss": 2.9329,
      "step": 6900000
    },
    {
      "epoch": 15.94,
      "grad_norm": 10.350345611572266,
      "learning_rate": 4.601528901833922e-05,
      "loss": 2.9332,
      "step": 6925000
    },
    {
      "epoch": 16.0,
      "grad_norm": 12.316056251525879,
      "learning_rate": 4.60009025707148e-05,
      "loss": 2.9339,
      "step": 6950000
    },
    {
      "epoch": 16.06,
      "grad_norm": 38.30925750732422,
      "learning_rate": 4.598651669870946e-05,
      "loss": 2.9318,
      "step": 6975000
    },
    {
      "epoch": 16.12,
      "grad_norm": 9.709274291992188,
      "learning_rate": 4.597213197794228e-05,
      "loss": 2.9282,
      "step": 7000000
    },
    {
      "epoch": 16.17,
      "grad_norm": 8.927230834960938,
      "learning_rate": 4.595774725717509e-05,
      "loss": 2.9298,
      "step": 7025000
    },
    {
      "epoch": 16.23,
      "grad_norm": 8.152135848999023,
      "learning_rate": 4.594336080955067e-05,
      "loss": 2.9338,
      "step": 7050000
    },
    {
      "epoch": 16.29,
      "grad_norm": 10.649055480957031,
      "learning_rate": 4.592897493754533e-05,
      "loss": 2.9322,
      "step": 7075000
    },
    {
      "epoch": 16.35,
      "grad_norm": 9.146815299987793,
      "learning_rate": 4.591459194363538e-05,
      "loss": 2.9286,
      "step": 7100000
    },
    {
      "epoch": 16.41,
      "grad_norm": 11.386180877685547,
      "learning_rate": 4.590020549601096e-05,
      "loss": 2.9341,
      "step": 7125000
    },
    {
      "epoch": 16.46,
      "grad_norm": 22.5006046295166,
      "learning_rate": 4.5885820199624695e-05,
      "loss": 2.9381,
      "step": 7150000
    },
    {
      "epoch": 16.52,
      "grad_norm": 11.183572769165039,
      "learning_rate": 4.587143605447659e-05,
      "loss": 2.9408,
      "step": 7175000
    },
    {
      "epoch": 16.58,
      "grad_norm": 8.468676567077637,
      "learning_rate": 4.585705075809033e-05,
      "loss": 2.9387,
      "step": 7200000
    },
    {
      "epoch": 16.64,
      "grad_norm": 8.212870597839355,
      "learning_rate": 4.5842665461704065e-05,
      "loss": 2.936,
      "step": 7225000
    },
    {
      "epoch": 16.69,
      "grad_norm": 8.824308395385742,
      "learning_rate": 4.58282801653178e-05,
      "loss": 2.9363,
      "step": 7250000
    },
    {
      "epoch": 16.75,
      "grad_norm": 9.485366821289062,
      "learning_rate": 4.5813894868931536e-05,
      "loss": 2.9438,
      "step": 7275000
    },
    {
      "epoch": 16.81,
      "grad_norm": 10.150970458984375,
      "learning_rate": 4.579950899692619e-05,
      "loss": 2.9344,
      "step": 7300000
    },
    {
      "epoch": 16.87,
      "grad_norm": 8.991164207458496,
      "learning_rate": 4.5785124276159015e-05,
      "loss": 2.9395,
      "step": 7325000
    },
    {
      "epoch": 16.92,
      "grad_norm": 9.29886531829834,
      "learning_rate": 4.577073840415367e-05,
      "loss": 2.9386,
      "step": 7350000
    },
    {
      "epoch": 16.98,
      "grad_norm": 10.486710548400879,
      "learning_rate": 4.575635483462464e-05,
      "loss": 2.9561,
      "step": 7375000
    },
    {
      "epoch": 17.04,
      "grad_norm": 10.57312297821045,
      "learning_rate": 4.5741968962619296e-05,
      "loss": 2.9671,
      "step": 7400000
    },
    {
      "epoch": 17.1,
      "grad_norm": 11.629875183105469,
      "learning_rate": 4.572758309061396e-05,
      "loss": 2.938,
      "step": 7425000
    },
    {
      "epoch": 17.15,
      "grad_norm": 28.40058708190918,
      "learning_rate": 4.5713198369846775e-05,
      "loss": 2.9485,
      "step": 7450000
    },
    {
      "epoch": 17.21,
      "grad_norm": 10.586529731750488,
      "learning_rate": 4.569881307346051e-05,
      "loss": 2.9946,
      "step": 7475000
    },
    {
      "epoch": 17.27,
      "grad_norm": 13.706501960754395,
      "learning_rate": 4.5684427777074246e-05,
      "loss": 3.1213,
      "step": 7500000
    },
    {
      "epoch": 17.33,
      "grad_norm": 13.260613441467285,
      "learning_rate": 4.567004248068798e-05,
      "loss": 3.1446,
      "step": 7525000
    },
    {
      "epoch": 17.38,
      "grad_norm": 9.9735746383667,
      "learning_rate": 4.565565718430172e-05,
      "loss": 3.1297,
      "step": 7550000
    },
    {
      "epoch": 17.44,
      "grad_norm": 13.090508460998535,
      "learning_rate": 4.5641272463534535e-05,
      "loss": 3.1027,
      "step": 7575000
    },
    {
      "epoch": 17.5,
      "grad_norm": 10.409151077270508,
      "learning_rate": 4.562688716714827e-05,
      "loss": 3.0725,
      "step": 7600000
    },
    {
      "epoch": 17.56,
      "grad_norm": 11.417243957519531,
      "learning_rate": 4.561250244638108e-05,
      "loss": 3.0268,
      "step": 7625000
    },
    {
      "epoch": 17.61,
      "grad_norm": 277.6300048828125,
      "learning_rate": 4.559811714999482e-05,
      "loss": 3.0053,
      "step": 7650000
    },
    {
      "epoch": 17.67,
      "grad_norm": 12.95353889465332,
      "learning_rate": 4.558373127798948e-05,
      "loss": 3.0845,
      "step": 7675000
    },
    {
      "epoch": 17.73,
      "grad_norm": 8.185384750366211,
      "learning_rate": 4.556934540598414e-05,
      "loss": 3.0831,
      "step": 7700000
    },
    {
      "epoch": 17.79,
      "grad_norm": 11.359471321105957,
      "learning_rate": 4.5554960109597875e-05,
      "loss": 3.0391,
      "step": 7725000
    },
    {
      "epoch": 17.84,
      "grad_norm": 10.606732368469238,
      "learning_rate": 4.554057538883069e-05,
      "loss": 2.9965,
      "step": 7750000
    },
    {
      "epoch": 17.9,
      "grad_norm": 9.946850776672363,
      "learning_rate": 4.552619009244443e-05,
      "loss": 3.0687,
      "step": 7775000
    },
    {
      "epoch": 17.96,
      "grad_norm": 9.408415794372559,
      "learning_rate": 4.551180537167724e-05,
      "loss": 3.0461,
      "step": 7800000
    },
    {
      "epoch": 18.02,
      "grad_norm": 9.750507354736328,
      "learning_rate": 4.54974194996719e-05,
      "loss": 2.9919,
      "step": 7825000
    },
    {
      "epoch": 18.07,
      "grad_norm": 9.908191680908203,
      "learning_rate": 4.5483034778904716e-05,
      "loss": 2.9642,
      "step": 7850000
    },
    {
      "epoch": 18.13,
      "grad_norm": 8.678967475891113,
      "learning_rate": 4.546864948251845e-05,
      "loss": 3.0119,
      "step": 7875000
    },
    {
      "epoch": 18.19,
      "grad_norm": 12.104811668395996,
      "learning_rate": 4.5454263610513107e-05,
      "loss": 3.0454,
      "step": 7900000
    },
    {
      "epoch": 18.25,
      "grad_norm": 11.42959213256836,
      "learning_rate": 4.543987831412684e-05,
      "loss": 3.038,
      "step": 7925000
    },
    {
      "epoch": 18.3,
      "grad_norm": 10.635873794555664,
      "learning_rate": 4.5425492442121504e-05,
      "loss": 3.0304,
      "step": 7950000
    },
    {
      "epoch": 18.36,
      "grad_norm": 8.923405647277832,
      "learning_rate": 4.5411106570116165e-05,
      "loss": 3.0552,
      "step": 7975000
    },
    {
      "epoch": 18.42,
      "grad_norm": 9.735569953918457,
      "learning_rate": 4.5396721849348975e-05,
      "loss": 3.06,
      "step": 8000000
    },
    {
      "epoch": 18.48,
      "grad_norm": 8.509729385375977,
      "learning_rate": 4.5382335977343637e-05,
      "loss": 3.0185,
      "step": 8025000
    },
    {
      "epoch": 18.53,
      "grad_norm": 16.212800979614258,
      "learning_rate": 4.536795068095737e-05,
      "loss": 3.043,
      "step": 8050000
    },
    {
      "epoch": 18.59,
      "grad_norm": 12.958589553833008,
      "learning_rate": 4.535356538457111e-05,
      "loss": 3.0248,
      "step": 8075000
    },
    {
      "epoch": 18.65,
      "grad_norm": 9.443342208862305,
      "learning_rate": 4.533917951256576e-05,
      "loss": 2.9926,
      "step": 8100000
    },
    {
      "epoch": 18.71,
      "grad_norm": 11.385446548461914,
      "learning_rate": 4.532479479179858e-05,
      "loss": 2.9914,
      "step": 8125000
    },
    {
      "epoch": 18.77,
      "grad_norm": 30.971221923828125,
      "learning_rate": 4.531040834417416e-05,
      "loss": 2.97,
      "step": 8150000
    },
    {
      "epoch": 18.82,
      "grad_norm": 8.520257949829102,
      "learning_rate": 4.529602362340698e-05,
      "loss": 3.0102,
      "step": 8175000
    },
    {
      "epoch": 18.88,
      "grad_norm": 9.118221282958984,
      "learning_rate": 4.528163890263979e-05,
      "loss": 2.9869,
      "step": 8200000
    },
    {
      "epoch": 18.94,
      "grad_norm": 9.861720085144043,
      "learning_rate": 4.526725360625353e-05,
      "loss": 2.9853,
      "step": 8225000
    },
    {
      "epoch": 19.0,
      "grad_norm": 8.959242820739746,
      "learning_rate": 4.525286773424819e-05,
      "loss": 2.9904,
      "step": 8250000
    },
    {
      "epoch": 19.05,
      "grad_norm": 8.833227157592773,
      "learning_rate": 4.5238483013481e-05,
      "loss": 2.9845,
      "step": 8275000
    },
    {
      "epoch": 19.11,
      "grad_norm": 9.430367469787598,
      "learning_rate": 4.5224097141475656e-05,
      "loss": 2.9949,
      "step": 8300000
    },
    {
      "epoch": 19.17,
      "grad_norm": 9.112401008605957,
      "learning_rate": 4.52097118450894e-05,
      "loss": 2.9913,
      "step": 8325000
    },
    {
      "epoch": 19.23,
      "grad_norm": 11.344586372375488,
      "learning_rate": 4.5195327124322215e-05,
      "loss": 2.9985,
      "step": 8350000
    },
    {
      "epoch": 19.28,
      "grad_norm": 8.723060607910156,
      "learning_rate": 4.518094067669779e-05,
      "loss": 3.0035,
      "step": 8375000
    },
    {
      "epoch": 19.34,
      "grad_norm": 9.955183982849121,
      "learning_rate": 4.5166555380311524e-05,
      "loss": 2.9947,
      "step": 8400000
    },
    {
      "epoch": 19.4,
      "grad_norm": 8.669763565063477,
      "learning_rate": 4.515217008392527e-05,
      "loss": 2.9483,
      "step": 8425000
    },
    {
      "epoch": 19.46,
      "grad_norm": 9.571410179138184,
      "learning_rate": 4.513778421191992e-05,
      "loss": 2.965,
      "step": 8450000
    },
    {
      "epoch": 19.51,
      "grad_norm": 9.903342247009277,
      "learning_rate": 4.512340006677181e-05,
      "loss": 2.9586,
      "step": 8475000
    },
    {
      "epoch": 19.57,
      "grad_norm": 8.95388126373291,
      "learning_rate": 4.5109014194766474e-05,
      "loss": 2.9853,
      "step": 8500000
    },
    {
      "epoch": 19.63,
      "grad_norm": 9.524025917053223,
      "learning_rate": 4.509462889838021e-05,
      "loss": 2.9595,
      "step": 8525000
    },
    {
      "epoch": 19.69,
      "grad_norm": 8.8157320022583,
      "learning_rate": 4.508024417761302e-05,
      "loss": 2.9456,
      "step": 8550000
    },
    {
      "epoch": 19.74,
      "grad_norm": 9.474971771240234,
      "learning_rate": 4.506585888122676e-05,
      "loss": 2.9496,
      "step": 8575000
    },
    {
      "epoch": 19.8,
      "grad_norm": 10.781895637512207,
      "learning_rate": 4.505147243360234e-05,
      "loss": 2.9516,
      "step": 8600000
    },
    {
      "epoch": 19.86,
      "grad_norm": 10.35008716583252,
      "learning_rate": 4.5037088864073316e-05,
      "loss": 2.9763,
      "step": 8625000
    },
    {
      "epoch": 19.92,
      "grad_norm": 9.373032569885254,
      "learning_rate": 4.502270299206797e-05,
      "loss": 2.965,
      "step": 8650000
    },
    {
      "epoch": 19.97,
      "grad_norm": 10.169739723205566,
      "learning_rate": 4.500831827130079e-05,
      "loss": 2.9829,
      "step": 8675000
    },
    {
      "epoch": 20.03,
      "grad_norm": 9.944557189941406,
      "learning_rate": 4.499393182367637e-05,
      "loss": 2.9651,
      "step": 8700000
    },
    {
      "epoch": 20.09,
      "grad_norm": 8.647862434387207,
      "learning_rate": 4.497954825414733e-05,
      "loss": 2.9503,
      "step": 8725000
    },
    {
      "epoch": 20.15,
      "grad_norm": 11.187058448791504,
      "learning_rate": 4.496516180652291e-05,
      "loss": 2.9494,
      "step": 8750000
    },
    {
      "epoch": 20.2,
      "grad_norm": 9.79581356048584,
      "learning_rate": 4.4950775934517575e-05,
      "loss": 2.9444,
      "step": 8775000
    },
    {
      "epoch": 20.26,
      "grad_norm": 8.577890396118164,
      "learning_rate": 4.4936390062512236e-05,
      "loss": 2.9331,
      "step": 8800000
    },
    {
      "epoch": 20.32,
      "grad_norm": 8.883890151977539,
      "learning_rate": 4.492200476612597e-05,
      "loss": 2.9455,
      "step": 8825000
    },
    {
      "epoch": 20.38,
      "grad_norm": 10.849936485290527,
      "learning_rate": 4.490762004535878e-05,
      "loss": 2.936,
      "step": 8850000
    },
    {
      "epoch": 20.43,
      "grad_norm": 8.091912269592285,
      "learning_rate": 4.489323417335344e-05,
      "loss": 2.9708,
      "step": 8875000
    },
    {
      "epoch": 20.49,
      "grad_norm": 8.828187942504883,
      "learning_rate": 4.487884887696718e-05,
      "loss": 2.9568,
      "step": 8900000
    },
    {
      "epoch": 20.55,
      "grad_norm": 11.910221099853516,
      "learning_rate": 4.486446300496184e-05,
      "loss": 2.9609,
      "step": 8925000
    },
    {
      "epoch": 20.61,
      "grad_norm": 8.431140899658203,
      "learning_rate": 4.485007943543281e-05,
      "loss": 2.9422,
      "step": 8950000
    },
    {
      "epoch": 20.66,
      "grad_norm": 11.341093063354492,
      "learning_rate": 4.483569413904655e-05,
      "loss": 2.9639,
      "step": 8975000
    },
    {
      "epoch": 20.72,
      "grad_norm": 11.073570251464844,
      "learning_rate": 4.48213082670412e-05,
      "loss": 2.9572,
      "step": 9000000
    },
    {
      "epoch": 20.78,
      "grad_norm": 10.323945045471191,
      "learning_rate": 4.480692354627402e-05,
      "loss": 2.9308,
      "step": 9025000
    },
    {
      "epoch": 20.84,
      "grad_norm": 10.437650680541992,
      "learning_rate": 4.479253882550684e-05,
      "loss": 2.9288,
      "step": 9050000
    },
    {
      "epoch": 20.89,
      "grad_norm": 8.601655960083008,
      "learning_rate": 4.47781529535015e-05,
      "loss": 2.9331,
      "step": 9075000
    },
    {
      "epoch": 20.95,
      "grad_norm": 10.499547004699707,
      "learning_rate": 4.4763767081496146e-05,
      "loss": 2.9372,
      "step": 9100000
    },
    {
      "epoch": 21.01,
      "grad_norm": 9.318188667297363,
      "learning_rate": 4.474938178510989e-05,
      "loss": 2.9392,
      "step": 9125000
    },
    {
      "epoch": 21.07,
      "grad_norm": 8.944506645202637,
      "learning_rate": 4.4734997064342706e-05,
      "loss": 2.9329,
      "step": 9150000
    },
    {
      "epoch": 21.13,
      "grad_norm": 9.7744779586792,
      "learning_rate": 4.4720610041099205e-05,
      "loss": 2.9425,
      "step": 9175000
    },
    {
      "epoch": 21.18,
      "grad_norm": 10.287466049194336,
      "learning_rate": 4.4706225895951096e-05,
      "loss": 2.9183,
      "step": 9200000
    },
    {
      "epoch": 21.24,
      "grad_norm": 8.818765640258789,
      "learning_rate": 4.469184117518391e-05,
      "loss": 2.9303,
      "step": 9225000
    },
    {
      "epoch": 21.3,
      "grad_norm": 8.111599922180176,
      "learning_rate": 4.4677454727559493e-05,
      "loss": 2.9336,
      "step": 9250000
    },
    {
      "epoch": 21.36,
      "grad_norm": 13.912071228027344,
      "learning_rate": 4.4663070582411385e-05,
      "loss": 2.9231,
      "step": 9275000
    },
    {
      "epoch": 21.41,
      "grad_norm": 9.105782508850098,
      "learning_rate": 4.4648684134786965e-05,
      "loss": 2.9327,
      "step": 9300000
    },
    {
      "epoch": 21.47,
      "grad_norm": 9.36768627166748,
      "learning_rate": 4.463429941401978e-05,
      "loss": 2.9385,
      "step": 9325000
    },
    {
      "epoch": 21.53,
      "grad_norm": 9.434181213378906,
      "learning_rate": 4.461991411763352e-05,
      "loss": 2.9473,
      "step": 9350000
    },
    {
      "epoch": 21.59,
      "grad_norm": 8.336790084838867,
      "learning_rate": 4.46055276700091e-05,
      "loss": 2.9204,
      "step": 9375000
    },
    {
      "epoch": 21.64,
      "grad_norm": 9.279216766357422,
      "learning_rate": 4.45911406467656e-05,
      "loss": 2.9129,
      "step": 9400000
    },
    {
      "epoch": 21.7,
      "grad_norm": 11.089231491088867,
      "learning_rate": 4.457675707723657e-05,
      "loss": 2.9156,
      "step": 9425000
    },
    {
      "epoch": 21.76,
      "grad_norm": 8.20855712890625,
      "learning_rate": 4.456237178085031e-05,
      "loss": 2.9162,
      "step": 9450000
    },
    {
      "epoch": 21.82,
      "grad_norm": 9.946846961975098,
      "learning_rate": 4.454798648446404e-05,
      "loss": 2.9089,
      "step": 9475000
    },
    {
      "epoch": 21.87,
      "grad_norm": 9.504951477050781,
      "learning_rate": 4.453360233931594e-05,
      "loss": 2.9121,
      "step": 9500000
    },
    {
      "epoch": 21.93,
      "grad_norm": 11.176216125488281,
      "learning_rate": 4.4519216467310594e-05,
      "loss": 2.9251,
      "step": 9525000
    },
    {
      "epoch": 21.99,
      "grad_norm": 9.329869270324707,
      "learning_rate": 4.450483117092433e-05,
      "loss": 2.9264,
      "step": 9550000
    },
    {
      "epoch": 22.05,
      "grad_norm": 10.256937980651855,
      "learning_rate": 4.4490445298918984e-05,
      "loss": 2.9028,
      "step": 9575000
    },
    {
      "epoch": 22.1,
      "grad_norm": 9.402801513671875,
      "learning_rate": 4.447606000253273e-05,
      "loss": 2.9133,
      "step": 9600000
    },
    {
      "epoch": 22.16,
      "grad_norm": 8.997471809387207,
      "learning_rate": 4.446167413052739e-05,
      "loss": 2.9117,
      "step": 9625000
    },
    {
      "epoch": 22.22,
      "grad_norm": 10.032304763793945,
      "learning_rate": 4.44472894097602e-05,
      "loss": 2.9182,
      "step": 9650000
    },
    {
      "epoch": 22.28,
      "grad_norm": 8.472091674804688,
      "learning_rate": 4.443290296213578e-05,
      "loss": 2.9148,
      "step": 9675000
    },
    {
      "epoch": 22.33,
      "grad_norm": 9.919981002807617,
      "learning_rate": 4.4418518241368595e-05,
      "loss": 2.9076,
      "step": 9700000
    },
    {
      "epoch": 22.39,
      "grad_norm": 8.974760055541992,
      "learning_rate": 4.440413236936326e-05,
      "loss": 2.9008,
      "step": 9725000
    },
    {
      "epoch": 22.45,
      "grad_norm": 10.042776107788086,
      "learning_rate": 4.438974707297699e-05,
      "loss": 2.9053,
      "step": 9750000
    },
    {
      "epoch": 22.51,
      "grad_norm": 9.80505084991455,
      "learning_rate": 4.437536120097165e-05,
      "loss": 2.9048,
      "step": 9775000
    },
    {
      "epoch": 22.56,
      "grad_norm": 9.193342208862305,
      "learning_rate": 4.436097590458538e-05,
      "loss": 2.9042,
      "step": 9800000
    },
    {
      "epoch": 22.62,
      "grad_norm": 8.377031326293945,
      "learning_rate": 4.434659060819912e-05,
      "loss": 2.9006,
      "step": 9825000
    },
    {
      "epoch": 22.68,
      "grad_norm": 10.352310180664062,
      "learning_rate": 4.433220473619378e-05,
      "loss": 2.898,
      "step": 9850000
    },
    {
      "epoch": 22.74,
      "grad_norm": 9.43801498413086,
      "learning_rate": 4.4317819439807516e-05,
      "loss": 2.8964,
      "step": 9875000
    },
    {
      "epoch": 22.79,
      "grad_norm": 9.111021041870117,
      "learning_rate": 4.430343414342125e-05,
      "loss": 2.8981,
      "step": 9900000
    },
    {
      "epoch": 22.85,
      "grad_norm": 9.87536907196045,
      "learning_rate": 4.428904999827315e-05,
      "loss": 2.9014,
      "step": 9925000
    },
    {
      "epoch": 22.91,
      "grad_norm": 9.640618324279785,
      "learning_rate": 4.427466470188688e-05,
      "loss": 2.8982,
      "step": 9950000
    },
    {
      "epoch": 22.97,
      "grad_norm": 9.849618911743164,
      "learning_rate": 4.426027882988154e-05,
      "loss": 2.9071,
      "step": 9975000
    },
    {
      "epoch": 23.02,
      "grad_norm": 9.269990921020508,
      "learning_rate": 4.424589238225712e-05,
      "loss": 2.9031,
      "step": 10000000
    },
    {
      "epoch": 23.08,
      "grad_norm": 18.298377990722656,
      "learning_rate": 4.423150823710902e-05,
      "loss": 2.9184,
      "step": 10025000
    },
    {
      "epoch": 23.14,
      "grad_norm": 9.27863597869873,
      "learning_rate": 4.421712178948459e-05,
      "loss": 2.9182,
      "step": 10050000
    },
    {
      "epoch": 23.2,
      "grad_norm": 11.538057327270508,
      "learning_rate": 4.420273706871741e-05,
      "loss": 2.9132,
      "step": 10075000
    },
    {
      "epoch": 23.25,
      "grad_norm": 11.701263427734375,
      "learning_rate": 4.418835119671206e-05,
      "loss": 2.9097,
      "step": 10100000
    },
    {
      "epoch": 23.31,
      "grad_norm": 11.924108505249023,
      "learning_rate": 4.4173965900325806e-05,
      "loss": 2.9337,
      "step": 10125000
    },
    {
      "epoch": 23.37,
      "grad_norm": 8.409440040588379,
      "learning_rate": 4.4159580603939535e-05,
      "loss": 2.9132,
      "step": 10150000
    },
    {
      "epoch": 23.43,
      "grad_norm": 8.634835243225098,
      "learning_rate": 4.4145194156315115e-05,
      "loss": 2.908,
      "step": 10175000
    },
    {
      "epoch": 23.49,
      "grad_norm": 9.207501411437988,
      "learning_rate": 4.413080885992886e-05,
      "loss": 2.8885,
      "step": 10200000
    },
    {
      "epoch": 23.54,
      "grad_norm": 10.840728759765625,
      "learning_rate": 4.4116424139161674e-05,
      "loss": 2.8967,
      "step": 10225000
    },
    {
      "epoch": 23.6,
      "grad_norm": 10.102667808532715,
      "learning_rate": 4.410203826715633e-05,
      "loss": 2.8897,
      "step": 10250000
    },
    {
      "epoch": 23.66,
      "grad_norm": 10.840648651123047,
      "learning_rate": 4.4087652970770065e-05,
      "loss": 2.8917,
      "step": 10275000
    },
    {
      "epoch": 23.72,
      "grad_norm": 8.944602012634277,
      "learning_rate": 4.4073266523145645e-05,
      "loss": 2.8881,
      "step": 10300000
    },
    {
      "epoch": 23.77,
      "grad_norm": 9.614134788513184,
      "learning_rate": 4.4058880075521225e-05,
      "loss": 2.8847,
      "step": 10325000
    },
    {
      "epoch": 23.83,
      "grad_norm": 10.729629516601562,
      "learning_rate": 4.4044495930373116e-05,
      "loss": 2.8848,
      "step": 10350000
    },
    {
      "epoch": 23.89,
      "grad_norm": 9.225468635559082,
      "learning_rate": 4.403011005836777e-05,
      "loss": 2.8858,
      "step": 10375000
    },
    {
      "epoch": 23.95,
      "grad_norm": 10.294960975646973,
      "learning_rate": 4.4015725337600595e-05,
      "loss": 2.8836,
      "step": 10400000
    },
    {
      "epoch": 24.0,
      "grad_norm": 10.987671852111816,
      "learning_rate": 4.400133946559525e-05,
      "loss": 2.8835,
      "step": 10425000
    },
    {
      "epoch": 24.06,
      "grad_norm": 8.421087265014648,
      "learning_rate": 4.398695359358991e-05,
      "loss": 2.879,
      "step": 10450000
    },
    {
      "epoch": 24.12,
      "grad_norm": 9.393680572509766,
      "learning_rate": 4.397256829720364e-05,
      "loss": 2.8843,
      "step": 10475000
    },
    {
      "epoch": 24.18,
      "grad_norm": 10.376935958862305,
      "learning_rate": 4.395818300081738e-05,
      "loss": 2.8829,
      "step": 10500000
    },
    {
      "epoch": 24.23,
      "grad_norm": 10.329286575317383,
      "learning_rate": 4.39437982800502e-05,
      "loss": 2.8869,
      "step": 10525000
    },
    {
      "epoch": 24.29,
      "grad_norm": 10.616833686828613,
      "learning_rate": 4.392941183242578e-05,
      "loss": 2.8876,
      "step": 10550000
    },
    {
      "epoch": 24.35,
      "grad_norm": 10.029775619506836,
      "learning_rate": 4.391502768727767e-05,
      "loss": 2.8976,
      "step": 10575000
    },
    {
      "epoch": 24.41,
      "grad_norm": 8.60482406616211,
      "learning_rate": 4.3900642390891407e-05,
      "loss": 2.9001,
      "step": 10600000
    },
    {
      "epoch": 24.46,
      "grad_norm": 11.26839828491211,
      "learning_rate": 4.388625651888607e-05,
      "loss": 2.8915,
      "step": 10625000
    },
    {
      "epoch": 24.52,
      "grad_norm": 9.557952880859375,
      "learning_rate": 4.387187064688072e-05,
      "loss": 2.8872,
      "step": 10650000
    }
  ],
  "logging_steps": 25000,
  "max_steps": 86863000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 200,
  "save_steps": 50000,
  "total_flos": 8.974672797164411e+19,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
